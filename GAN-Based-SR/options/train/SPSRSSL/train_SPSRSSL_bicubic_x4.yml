name: SPSRSSL_bicubic_x4
model_type: SPSRSSLModel
scale: 4
num_gpu: auto  # set num_gpu: 0 for cpu mode
manual_seed: 0

pre_pad: 0
tile_size: 200
tile_pad: 32
tile_process: False

# dataset and data loader settings
datasets:
  train:
    name: DIV2K
    type: PairedImageMaskDataset
    dataroot_gt: /data1_ssd4t/chendu/datasets/DIV2K/DIV2K_train_multiscale_HR_subimages_512
    dataroot_lq: /data1_ssd4t/chendu/datasets/DIV2K/bicubicLR/x4/DIV2K_train_multiscale_HR_subimages_512
    dataroot_gt_mask: /data1_ssd4t/chendu/datasets/DIV2K/mask_selfsim/DIV2K_train_multiscale_HR_subimages_512/Laplacian/L/threshold-20.0/mat
    # (for lmdb)
    # dataroot_gt: datasets/DIV2K/DIV2K_train_HR_sub.lmdb
    # dataroot_lq: datasets/DIV2K/DIV2K_train_LR_bicubic_X4_sub.lmdb
    filename_tmpl: '{}'
    io_backend:
      type: disk
      # (for lmdb)
      # type: lmdb

    gt_size: 128
    use_hflip: true
    use_rot: true

    # data loader
    num_worker_per_gpu: 15
    batch_size_per_gpu: 30
    dataset_enlarge_ratio: 1
    prefetch_mode: ~

  val:
    name: DIV2K100
    type: PairedImageDataset
    dataroot_gt: /home/chendu/data2_hdd10t/chendu/dataset/basicsr/Set5/GT/GTmod12
    dataroot_lq: /home/chendu/data2_hdd10t/chendu/dataset/basicsr/Set5/bicubicLR/x4mod12
    io_backend:
      type: disk

# network structures
network_g:
  type: SPSRNet
  norm_type: null
  mode: "CNA"
  nf: 64
  nb: 23
  in_nc: 3
  out_nc: 3
  gc: 32

network_d:
  type: VGGStyleDiscriminator
  num_in_ch: 3
  num_feat: 64
  input_size: 128

network_d_grad:
  type: VGGStyleDiscriminator
  num_in_ch: 3
  num_feat: 64
  input_size: 128

# path
load_mode_g: my_pretrain
path:
  pretrain_network_g: experiments/pretrained_models/SPSR-PSNR/net_g.pth
  strict_load_g: true
  param_key_g: params_ema

ssl_setting:
  mask_stride: 3
  ssl_mode: 'cuda'  # You could change it to "pytorch" to see the detailed implementation of our SSL. But we don't recommend you to do that since the pytorch version will need at leat 48GB GPU memory
  kernel_size_search: 25
  sigma: 0.004  # scaling_factor in the paper
  kernel_size_window: 9
  generalization: True

# training settings
train:
  Branch_pretrain: False
  Branch_init_iters: 5000
  ema_decay: 0.999
  optim_g:
    type: Adam
    lr: !!float 1e-4
    weight_decay: 0
    betas: [0.9, 0.99]
  optim_d:
    type: Adam
    lr: !!float 1e-4
    weight_decay: 0
    betas: [0.9, 0.99]
  optim_d_grad:
    type: Adam
    lr: !!float 1e-4
    weight_decay: 0
    betas: [0.9, 0.99]

  scheduler:
    type: MultiStepLR
    milestones: [50000, 100000, 200000, 300000]
    gamma: 0.5

  total_iter: 400000
  warmup_iter: -1  # no warm up

  # losses
  pixel_opt:
    type: L1Loss
    loss_weight: !!float 1e-2
    reduction: mean
  pixel_gradSR_opt:
    type: MSELoss
    loss_weight: !!float 1e-2
    reduction: mean
  pixel_gradBranch_opt:
    type: L1Loss
    loss_weight: !!float 5e-1
    reduction: mean
  selfsim_opt:
    type: L1Loss
    loss_weight: !!float 1e3
    reduction: mean
  selfsim1_opt:
    type:  KLDistanceLoss
    loss_weight: !!float 1e3
    reduction: mean
    softmax: False
  perceptual_opt:
    type: PerceptualLoss
    layer_weights:
      'conv5_4': 1  # before relu
    vgg_type: vgg19
    use_input_norm: true
    range_norm: false
    perceptual_weight: 1.0
    style_weight: 0
    criterion: l1
  gan_opt:
    type: GANLoss
    gan_type: vanilla
    real_label_val: 1.0
    fake_label_val: 0.0
    loss_weight: !!float 5e-3
  gan_gradSR_opt:
    type: GANLoss
    gan_type: vanilla
    real_label_val: 1.0
    fake_label_val: 0.0
    loss_weight: !!float 5e-3

  net_d_iters: 1
  net_d_init_iters: 0

# validation settings
val:
  val_freq: !!float 2e3
  save_img: ~

  metrics:
    psnr: # metric name, can be arbitrary
      type: calculate_psnr
      crop_border: 4
      test_y_channel: True
    ssim: # metric name
      type: calculate_ssim
      crop_border: 4
      test_y_channel: True
    lpips:
      type: calculate_lpips
      crop_border: 4
      better: lower
    dists:
      type: calculate_dists
      crop_border: 4
      better: lower

# logging settings
logger:
  print_freq: 100
  save_checkpoint_freq: !!float 2e3
  use_tb_logger: true
  wandb:
    project: ~
    resume_id: ~

# dist training settings
dist_params:
  backend: nccl
  port: 29500
