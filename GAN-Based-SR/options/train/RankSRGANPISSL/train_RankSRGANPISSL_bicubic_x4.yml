name: RankSRGANPISSL_bicubic_x4
model_type: RankSRGANSSLModel
scale: 4
num_gpu: auto  # set num_gpu: 0 for cpu mode
manual_seed: 0

pre_pad: 0
tile_size: 800
tile_pad: 32
tile_process: False

# dataset and data loader settings
datasets:
  train:
    name: DIV2K
    type: PairedImageMaskDataset
    dataroot_gt: /data1_ssd4t/chendu/datasets/DIV2K/DIV2K_train_multiscale_HR_subimages_512
    dataroot_lq: /data1_ssd4t/chendu/datasets/DIV2K/bicubicLR/x4/DIV2K_train_multiscale_HR_subimages_512
    dataroot_gt_mask: /data1_ssd4t/chendu/datasets/DIV2K/mask_selfsim/DIV2K_train_multiscale_HR_subimages_512/Laplacian/L/threshold-20.0/mat
    # (for lmdb)
    # dataroot_gt: datasets/DIV2K/DIV2K_train_HR_sub.lmdb
    # dataroot_lq: datasets/DIV2K/DIV2K_train_LR_bicubic_X4_sub.lmdb
    filename_tmpl: '{}'
    io_backend:
      type: disk
      # (for lmdb)
      # type: lmdb

    gt_size: 296
    use_hflip: true
    use_rot: true

    # data loader
    num_worker_per_gpu: 4
    batch_size_per_gpu: 32
    dataset_enlarge_ratio: 1
    prefetch_mode: ~

  val:
    name: DIV2K100
    type: PairedImageDataset
    dataroot_gt: /home/chendu/data2_hdd10t/chendu/dataset/basicsr/DIV2K100/GT/GTmod12
    dataroot_lq: /home/chendu/data2_hdd10t/chendu/dataset/basicsr/DIV2K100/bicubicLR/x4mod12
    io_backend:
      type: disk

# network structures
network_g:
  type: RankSRGANSRResNet
  in_nc: 3
  out_nc: 3
  nf: 64
  nb: 16
  upscale: 4

network_r:
  type: Ranker_VGG12_296
  in_nc: 3
  nf: 64

network_d:
  type: Discriminator_VGG_296
  in_nc: 3
  nf: 64

# path
load_mode_g: original
path:
  pretrain_network_g: experiments/pretrained_models/RankSRGAN/mmsr_SRResNet_pretrain.pth

load_mode_r: original
path_r:
  pretrain_network_r: experiments/pretrained_models/RankSRGAN/mmsr_Ranker_PI.pth

ssl_setting:
  mask_stride: 3
  ssl_mode: 'cuda'  # You could change it to "pytorch" to see the detailed implementation of our SSL. But we don't recommend you to do that since the pytorch version will need at leat 48GB GPU memory
  kernel_size_search: 25
  sigma: 0.004  # scaling_factor in the paper
  kernel_size_window: 9
  generalization: True

# training settings
train:
  gan_loss_compute: 'GAN'
  ema_decay: 0.999
  optim_g:
    type: Adam
    lr: !!float 1e-4
    weight_decay: 0
    betas: [0.9, 0.99]
  optim_d:
    type: Adam
    lr: !!float 1e-4
    weight_decay: 0
    betas: [0.9, 0.99]

  scheduler:
    type: MultiStepLR
    milestones: [50000, 100000, 200000, 300000]
    gamma: 0.5

  total_iter: 500000
  warmup_iter: -1  # no warm up

  # losses
  pixel_opt:
    type: L1Loss
    loss_weight: !!float 1e-2
    reduction: mean
  rank_opt:
    use_Ranker: True
    R_bias: 0
    loss_weight: !!float 3e-2
  selfsim_opt:
    type: L1Loss
    loss_weight: !!float 1e3
    reduction: mean
  selfsim1_opt:
    type:  KLDistanceLoss
    loss_weight: !!float 1e3
    reduction: mean
    softmax: False
  perceptual_opt:
    type: PerceptualLoss
    layer_weights:
      'conv5_4': 1  # before relu
    vgg_type: vgg19
    use_input_norm: true
    range_norm: false
    perceptual_weight: 1.0
    style_weight: 0
    criterion: l1
  gan_opt:
    type: GANLoss
    gan_type: vanilla
    real_label_val: 1.0
    fake_label_val: 0.0
    loss_weight: !!float 5e-3

  net_d_iters: 1
  net_d_init_iters: 0

# validation settings
val:
  val_freq: !!float 2e3
  save_img: ~

  metrics:
    psnr: # metric name, can be arbitrary
      type: calculate_psnr
      crop_border: 4
      test_y_channel: True
    ssim: # metric name
      type: calculate_ssim
      crop_border: 4
      test_y_channel: True
    lpips:
      type: calculate_lpips
      crop_border: 4
      better: lower
    dists:
      type: calculate_dists
      crop_border: 4
      better: lower

# logging settings
logger:
  print_freq: 100
  save_checkpoint_freq: !!float 2e3
  use_tb_logger: true
  wandb:
    project: ~
    resume_id: ~

# dist training settings
dist_params:
  backend: nccl
  port: 29500
